# EDGAR Utilities Module

## `lookup_region` function

Your `edgar_utils.py` should have an `lookup_region` function that takes an
IP address (in string form) and returns the region the corresponding computer is in.  It should use the `ip2location.csv` (borrowed from
https://lite.ip2location.com/database/ip-country) to lookup what
country/region owns a particular IP address. The CSV file looks like
this:

```
low,high,code,region
0,16777215,-,-
16777216,16777471,US,United States of America
16777472,16778239,CN,China
16778240,16779263,AU,Australia
16779264,16781311,CN,China
...
```

The first two numbers are IP ranges (inclusive on both ends).  For example, the IP
address 16777473 belongs to China because it is between 16777472 and
16778239.

IP addresses are more commonly represented as four-part numbers, like
"34.67.75.25".  To convert an address like this to an integer, you can
use the following:

```python
import netaddr
int(netaddr.IPAddress("34.67.75.25"))
```

In order to enable us to use this with the anonymized IPs in EDGAR logs, you will need to first process the anonymized IPs.  Some digits are censored as random letters in the dataset.
Any such letters can be replaced with zeros for simplicity (for
example, `104.197.32.ihd` becomes `104.197.32.000`) prior to int conversion.

Because this function will be used many times while processing the logs, it is important to make it fast. Because the IPs in `ip2location.csv` are ordered (both in the `low` and `high` columns), do this with binary search.

Binary search is a very efficient algorithm for searching a sorted list; by taking advantage of that information, it can be much faster than a sequential search (which checks each element, one by one). The difference in speed is dramatic: when searching a sorted array of a billion items, binary search will be tens of millions of times faster.

To apply binary search, use the `searchsorted` method from `numpy` (`numpy` is a very powerful library we'll learn more about later in the semester). The `to_numpy()` method of a column of a `DataFrame` returns a `numpy` array with the contents of the column (the `values` attribute may be used as well); this is actually how numerical `DataFrame` columns are represented under the hood (so the conversion is essentially free).

`array.searchsorted(value)` returns the index of the first item larger than `value`; by applying this to one of the `low` or `high` columns of the ip lookup table in `numpy` form, you can find the index of the range the ip belongs to; by indexing into the table, you can then find the region.

If you're interested, take a look at [the details](https://en.wikipedia.org/wiki/Binary_search_algorithm); implementing it yourself would be a good exercise (but is not required).

Example usage:
```python
>>> lookup_region("1.1.1.x")
'United States of America'
>>> lookup_region("101.1.1.abc")
'China'
```

## `Filing` class

We'll use **regular expressions** (available in Python's `re` module) to extract information from the EDGAR filings. `Filing` should only be used to process ".html" or ".htm" files; however, the processing should be done with regular expressions only (don't use BeautifulSoup) unless specified otherwise. When you're working on processing the files, remember that HTML is just text! Think of the file as a string, and look for patterns which can help you find the information you want.

The processing should be done in a `Filing` class in `edgar_utils.py` of the following form. 

```python
class Filing:
    filing_date_regex = re.compile(???)
    ...

    def __init__(self, path):
        # read the page source from the file with path "path" in docs.zip
        with ZipFile(???) as zf:
            with zf.open(???) as f:
                self.html = ???
        
        # extract the data from the page source
        self.filing_date = ???
        ...
```

Because the regular expressions will be used repeatedly, compiling them once before use with `re.compile(...)` is recommended; make sure this is done only once and not each time a new `Filing` object is created; storing the compiled regular expressions in class variables would be appropriate.

If you name your regular expressions carefully, you might find that `getattr` and `setattr` can be used to cut down on repeated code. Also, since Filing should extract all the information of interest by default, try to process all the ".html" and ".htm" documents in "docs.zip" at the start of your notebook; this will let you re-use the results multiple times.

When processing the files in "docs.zip", make sure to avoid attempting to process the directories. You might find the `filelist` attribute of a `ZipFile` helpful. It returns a list of `ZipInfo` objects, one for each file. Use the `ZipInfo`s `is_dir()` methods to check if the file is a directory, and get its path with the `filename` attribute.

Begin by implementing regular expressions to extract the following information.

- Filing date (store the result in attribute `filing_date` of a `Filing`)
- Date the filing was  accepted (store in attribute `accepted_date`)
- The state of incorporation of the filer (store it in `state_of_incorporation`)

Note: a few of the states of incorporation will not be US states.

**Hints:** Read about the non-greedy * operator (*?); it might be very helpful. 

Using "[^<]" in your regular expressions might also be useful; this will recognise any character except a "<", which is used at the beginning of a tag; "[^>]" could also be useful - it matches any character except a ">" (used at the end of a tag). Both of these might help you write more compact expressions.

Finally, look up `re.DOTALL`; it might be useful for some of the expressions.

## Extract corporate information

Write a regular expression to find the name of the filing company (note that the filing company will appear before other filers such as the legal firms hired by the filer). Store the result in the attribute `company_name` of a `Filing`.

Next, find the industry of the filing company and store it in the attribute `industry`. Note that this isn't shown for some types of filings (for example, investment prospectuses); store "unknown" if not found.

## Extract addresses

Extract the mailing address of the filing company (remember that the primary filer will be before secondary filers in the page). For this component it is not necessary to use a single regular expression; using several regular expressions and non-regex Python is acceptable here (and suggested). Store it in the attribute `address`.

**Hint:** Try to first find the tags containing the address lines (using the non-greedy * operator (*?) might make this much easier; this might also be a good place to use `re.DOTALL`), and next to extract the contents of each of those tags. Get the address by joining the lines together.

Next, find the state abbreviation in the address (remember that the state is prior to the zip code in the last line of the address). Store the result in `state_of_headquarters`. If there is no zip code in the last line, you may assume that the address is not in the US (and does not have a state); store "unknown" in this case.

US-standard zip codes are five digits, optionally followed by a dash and four digits.

Note: a few of the non-US addresses are formatted in a way which is consistent with the US format; this is expected. We could use a list of US state abbreviations to filter them out reliably, but we'll keep them, as the information might be useful.
